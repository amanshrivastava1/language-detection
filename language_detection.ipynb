{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_langs(\"The definition of multilingualism is a subject of debate in the same way as that of language fluency. On one end of a sort of linguistic continuum, one may define multilingualism as complete competence and mastery in another language. The speaker would presumably have complete knowledge and control over the language to sound native. On the opposite end of the spectrum would be people who know enough phrases to get around as a tourist using the alternate language. Since 1992, Vivian Cook has argued that most multilingual speakers fall somewhere between minimal and maximal definitions.Cook calls these people multi-competent Bonne après-midi Je suis ravi de vous rencontrer Je suis désolé Maman m'a dit la vie n'est pas facile Donc je souris quand j'ai mal à la vie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_langs(\"Je suis désolé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install langid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import langid\n",
    "L = [\"Geeksforgeeks is a computer science portal for geeks\", \n",
    "    \"Geeksforgeeks - это компьютерный портал для гиков\", \n",
    "    \"Geeksforgeeks es un portal informático para geeks\", \n",
    "    \"Geeksforgeeks是面向极客的计算机科学门户\", \n",
    "    \"Geeksforgeeks geeks के लिए एक कंप्यूटर विज्ञान पोर्टल है\", \n",
    "    \"Geeksforgeeksは、ギーク向けのコンピューターサイエンスポータルです。\", \n",
    "] \n",
    "  \n",
    "for i in L: \n",
    "      \n",
    "    # Language detection \n",
    "    print(langid.classify(i)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L = [\"Geeksforgeeks is a computer science portal for geeks Geeksforgeeks - ギーク向けのコンピューターサイエンスポータルです。 это компьютерный портал для гиков Geeksforgeeks是面向极客的计算机科学门户\"] \n",
    "for i in L:\n",
    "    print(langid.classify(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from langdetect import detect_langs\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "\n",
    "def _detect_language(spacy_object):\n",
    "    try:\n",
    "        detected_language = detect_langs(spacy_object.text)[0]\n",
    "        return {\"language\": str(detected_language.lang), \"score\": float(detected_language.prob)}\n",
    "    except LangDetectException:\n",
    "        return {\"language\": \"UNKNOWN\", \"score\": 0.0}\n",
    "\n",
    "\n",
    "class LanguageDetector(object):\n",
    "    \"\"\"Fully customizable language detection pipeline for spaCy.\n",
    "    Arguments:\n",
    "        language_detection_function: An optional custom language_detection_function. (Default None).\n",
    "                                     If None uses, langdetect package to detect language\n",
    "    # writing a custom language_detection_function:\n",
    "        The function must take in a spacy Doc or Span object only as input and can return the detected language.\n",
    "        This is stored in Doc._.language, Span._.language and Token._.language attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, language_detection_function=None):\n",
    "        if not language_detection_function:\n",
    "            self._language_detection_function = _detect_language\n",
    "        else:\n",
    "            self._language_detection_function = language_detection_function\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        assert isinstance(doc, Doc), \"doc must be an instance of spacy Doc. But got a {}\".format(type(doc))\n",
    "        doc.set_extension(\"language\", getter=self._language_detection_function, force=True)\n",
    "        for sent in doc.sents:\n",
    "            sent.set_extension(\"language\", getter=self._language_detection_function, force=True)\n",
    "        for token in doc:\n",
    "            token.set_extension(\"language\", getter=self._language_detection_function, force=True)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy_langdetect import LanguageDetector\n",
    "# install using pip install googletrans\n",
    "from googletrans import Translator\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def custom_detection_function(spacy_object):\n",
    "    # custom detection function should take a Spacy Doc or a\n",
    "    assert isinstance(spacy_object, Doc) or isinstance(\n",
    "        spacy_object, Span), \"spacy_object must be a spacy Doc or Span object but it is a {}\".format(type(spacy_object))\n",
    "    detection = Translator().detect(spacy_object.text)\n",
    "    return {'language':detection.lang, 'score':detection.confidence}\n",
    "\n",
    "nlp.add_pipe(LanguageDetector(language_detection_function=custom_detection_function), name=\"language_detector\", last=True)\n",
    "text = \"This is English text. Er lebt mit seinen Eltern und seiner Schwester in Berlin. Yo me divierto todos los días en el parque. Je m'appelle Angélica Summer, j'ai 12 ans et je suis canadienne.\"\n",
    "doc = nlp(text)\n",
    "# document level language detection. Think of it like average language of document!\n",
    "print(doc._.language)\n",
    "# sentence level language detection\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    print(sent, sent._.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
    "text = \"This is English text. Er lebt mit seinen Eltern und seiner Schwester in Berlin. Yo me divierto todos los días en el parque. Je m'appelle Angélica Summer, j'ai 12 ans et je suis canadienne.\"\n",
    "doc = nlp(text)\n",
    "# document level language detection. Think of it like average language of document!\n",
    "print(doc._.language)\n",
    "# sentence level language detection\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    print(sent, sent._.language)\n",
    "\n",
    "# Token level language detection from version 0.1.2\n",
    "# Use this with caution because, in some cases language detection will not make sense for individual tokens\n",
    "for token in doc:\n",
    "    print(token, token._.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install spacy_langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install tesserocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tesserocr import PyTessBaseAPI\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser('C:/Users/hp/')\n",
    "parser.add_argument('-i' , '--b.jpg' , help='C:/Users/hp/')\n",
    "#args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "images = ['b.jpg']\n",
    "images[0] = args.image\n",
    "count = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "with PyTessBaseAPI() as api:\n",
    "    for img in images:\n",
    "        api.Init(lang = 'hin')\n",
    "        api.SetImageFile(img)\n",
    "        # print api.AllWordConfidences()\n",
    "        arr = list(api.AllWordConfidences())\n",
    "        sumarr = sum(arr) / float(len(arr))\n",
    "\n",
    "\n",
    "with PyTessBaseAPI() as api:\n",
    "    for img in images:\n",
    "        api.Init(lang = 'eng')\n",
    "        api.SetImageFile(img)\n",
    "        # print api.AllWordConfidences()\n",
    "        arr2 = list(api.AllWordConfidences())\n",
    "        sumarr2 = sum(arr2) / float(len(arr2))\n",
    "\n",
    "\n",
    "\n",
    "with PyTessBaseAPI() as api:\n",
    "    for img in images:\n",
    "        api.Init(lang = 'spa')\n",
    "        api.SetImageFile(img)\n",
    "        # print api.AllWordConfidences()\n",
    "        arr3 = list(api.AllWordConfidences())\n",
    "        sumarr3 = sum(arr3) / float(len(arr3))\n",
    "\n",
    "\n",
    "n = min(len(arr) , len(arr2) , len(arr3))\n",
    "for i in range(0 , n):\n",
    "    if (arr[i] > arr2[i]) & (arr[i] > arr3[i]):\n",
    "        count += 1\n",
    "    elif (arr2[i] > arr[i]) & (arr2[i] > arr3[i]):\n",
    "        count2 += 1\n",
    "    elif (arr3[i] > arr[i]) & (arr3[i] > arr2[i]):\n",
    "        count3 += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "if (count3 > count2) & (count3 > count):\n",
    "        print(\"Spanish\")\n",
    "        print(\"Confidence score is \" + str(sumarr3))\n",
    "        api.Init(lang = 'spa')\n",
    "        api.SetImageFile(images[0])\n",
    "elif (count2 > count) & (count2 > count3):\n",
    "        print(\"English\")\n",
    "        print(\"Confidence score is \" + str(sumarr2))\n",
    "        api.Init(lang = 'eng')\n",
    "        api.SetImageFile(images[0])\n",
    "else:\n",
    "        print (\"Hindi\")\n",
    "        print(\"Confidence score is \" + str(sumarr))\n",
    "        api.Init(lang = 'hin')\n",
    "        api.SetImageFile(images[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tesserocr import PyTessBaseAPI\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(\"Enter Image Path\")\n",
    "parser.add_argument('-i' , '--image' , help=\"Image Path\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "#args = vars(parser.parse_args())\n",
    "\n",
    "images = ['b.jpg']\n",
    "images[0] = args.image\n",
    "count = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "with PyTessBaseAPI() as api:\n",
    "    for img in images:\n",
    "        api.Init(lang = 'hin')\n",
    "        api.SetImageFile(img)\n",
    "        # print api.AllWordConfidences()\n",
    "        arr = list(api.AllWordConfidences())\n",
    "        sumarr = sum(arr) / float(len(arr))\n",
    "\n",
    "\n",
    "with PyTessBaseAPI() as api:\n",
    "    for img in images:\n",
    "        api.Init(lang = 'eng')\n",
    "        api.SetImageFile(img)\n",
    "        # print api.AllWordConfidences()\n",
    "        arr2 = list(api.AllWordConfidences())\n",
    "        sumarr2 = sum(arr2) / float(len(arr2))\n",
    "\n",
    "\n",
    "\n",
    "with PyTessBaseAPI() as api:\n",
    "    for img in images:\n",
    "        api.Init(lang = 'spa')\n",
    "        api.SetImageFile(img)\n",
    "        # print api.AllWordConfidences()\n",
    "        arr3 = list(api.AllWordConfidences())\n",
    "        sumarr3 = sum(arr3) / float(len(arr3))\n",
    "\n",
    "\n",
    "n = min(len(arr) , len(arr2) , len(arr3))\n",
    "for i in range(0 , n):\n",
    "    if (arr[i] > arr2[i]) & (arr[i] > arr3[i]):\n",
    "        count += 1\n",
    "    elif (arr2[i] > arr[i]) & (arr2[i] > arr3[i]):\n",
    "        count2 += 1\n",
    "    elif (arr3[i] > arr[i]) & (arr3[i] > arr2[i]):\n",
    "        count3 += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "if (count3 > count2) & (count3 > count):\n",
    "        print(\"Spanish\")\n",
    "        print(\"Confidence score is \" + str(sumarr3))\n",
    "        api.Init(lang = 'spa')\n",
    "        api.SetImageFile(images[0])\n",
    "elif (count2 > count) & (count2 > count3):\n",
    "        print(\"English\")\n",
    "        print(\"Confidence score is \" + str(sumarr2))\n",
    "        api.Init(lang = 'eng')\n",
    "        api.SetImageFile(images[0])\n",
    "else:\n",
    "        print (\"Hindi\")\n",
    "        print(\"Confidence score is \" + str(sumarr))\n",
    "        api.Init(lang = 'hin')\n",
    "        api.SetImageFile(images[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -m site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " from tesserocr import PyTessBaseAPI\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(\"Enter Image Path\")\n",
    "parser.add_argument('-i' , '--image' , help=\"Image Path\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "#args = vars(parser.parse_args())\n",
    "\n",
    "images = ['b.jpg']\n",
    "images[0] = args.image\n",
    "count = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "with PyTessBaseAPI() as api:\n",
    "    for img in images:\n",
    "        api.Init(lang = 'eng')\n",
    "        api.SetImageFile(img)\n",
    "        # print api.AllWordConfidences()\n",
    "        arr = list(api.AllWordConfidences())\n",
    "        sumarr = sum(arr) / float(len(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tesserocr --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual document clusters discovery\n",
      "\n",
      "Benoit Mathieu & Romaric Besancon & Christian Fluhr\n",
      "CEA-LIC2M.\n",
      "B.P.6 92265 Fontenay-aux-roses Cedex France\n",
      "{mathieub,besanconr,flurhc} @zoe.cea.fr\n",
      "\n",
      "Abstract\n",
      "\n",
      "Cross Language Information Retrieval community has brought up search engines over multilingual\n",
      "corpora, and multilingual text categorization systems. In this paper, we focus on the multilingual clusters\n",
      "discovery problem, which aim is to extract topic-related multilingual document clusters from a\n",
      "multilingual document collection in an unsupervised way. Our approach is based on a linguistic analysis\n",
      "of the documents that allows to identify relevant features for a vector representation of the documents,\n",
      "cach language being associated with a different vector space. We propose a cross-lingual similarity\n",
      "‘measure for the documents, using bilingual dictionaries. A Shared Nearest Neighbor clustering algorithm\n",
      "is then used to build the clusters. We present an evaluation framework for this task, analyze and discuss\n",
      "the results we obtained and propose directions for future works.\n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "Résumé\n",
      "\n",
      "En recherche d'information multilingue, beaucoup de travaux ont été menés sur les moteurs de recherche\n",
      "et sur les systémes de catégorisation automatique de texte. Dans cet article, nous abordons le probleme de\n",
      "découverte automatique de classes de documents dans une collection multilingue. Le but est d'extraire des\n",
      "documents de langues différentes traitant d'un méme sujet. Notre approche se fonde sur une analyse\n",
      "linguistique des documents permettant dobtenir une représentation vectorielle des documents, chaque\n",
      "langue étant représentée dans un espace vectorel différent, Nous proposons une mesure de similarté\n",
      "\n",
      "igue sur ces documents, utilisant des dictionnaires bilingues. Un algorithme de classification du\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "type ‘plus proches voisins’ est alors utilisé pour construire les classes. Nous présentons également une\n",
      "methode dévaluation ainsi que les résultats obtenus. Ces résultats seromt Giscutés et nous envisagerons\n",
      "plusieurs axes d’amélioration du systéme.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Information technology and globalization generate more and more electronic documents written in\n",
      "different languages. Dealing with such an amount of data requires automatic systems to filter, retrieve and\n",
      "classify multilingual documents. The Cross Language Information Retrieval community has designed\n",
      "multilingual search engines and text categorization systems that help users with specific needs. Search\n",
      "engines retrieve documents related to a specific user need expressed by a query, text categorization\n",
      "systems are used to assign to each document one of several predefined categories.\n",
      "\n",
      "Several approaches have been considered to solve the cross-language information retrieval problem.\n",
      "Search engines translate the user query in all indexed languages, then retrieve documents of different\n",
      "languages and merge results (Savoy J. 2002; Fluhr C. et al. 1997). Query translation can be achieved by\n",
      "the simple use of dictionaries (no disambiguation), or by machine translation (with disambiguation). An\n",
      "alternative way for query translation is Latent Semantic Indexing (Littman M. et al. 1997), which\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import pytesseract \n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Users/hp/Anaconda3/tesseract.exe'\n",
    "\n",
    "img = cv2.imread('b1.png')\n",
    "text1 = pytesseract.image_to_string(img)\n",
    "\n",
    "text = [text1]\n",
    "print(text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----OPtional Part----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized to see the difference in direct text and after tokenize what is the type\n",
    "import nltk\n",
    "sentence = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[en:0.714283263286287, fr:0.28571631383832125]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Same can be used to detect multiple language in our text data\n",
    "# here text1 is in string format and text is in list (as we have converted to list format)\n",
    "# but here langdetect can directly detect the lang in string type we dont need to convert it\n",
    "# Text blob will work on the list type and not the string type\n",
    "\n",
    "from langdetect import detect_langs\n",
    "detect_langs(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text: \n",
    "      \n",
    "    # Language Detection \n",
    "    lang = TextBlob(i)  \n",
    "    print(lang.detect_language())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import langid \n",
    "\n",
    "for i in text1: \n",
    "      \n",
    "    # Language detection \n",
    "    print(langid.classify(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = re.search('(?<=Script: )\\d+', osd).group(0)\n",
    "print(\"angle: \", angle)\n",
    "print(\"script: \", script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual document clusters discovery\n",
      "\n",
      "Benoit Mathieu & Romaric Besancon & Christian Fluhr\n",
      "CEA-LIC2M.\n",
      "B.P.6 92265 Fontenay-aux-roses Cedex France\n",
      "{mathieub,besanconr,flurhc} @zoe.cea.fr\n",
      "\n",
      "Abstract\n",
      "\n",
      "Cross Language Information Retrieval community has brought up search engines over multilingual\n",
      "corpora, and multilingual text categorization systems. In this paper, we focus on the multilingual clusters\n",
      "discovery problem, which aim is to extract topic-related multilingual document clusters from a\n",
      "multilingual document collection in an unsupervised way. Our approach is based on a linguistic analysis\n",
      "of the documents that allows to identify relevant features for a vector representation of the documents,\n",
      "cach language being associated with a different vector space. We propose a cross-lingual similarity\n",
      "‘measure for the documents, using bilingual dictionaries. A Shared Nearest Neighbor clustering algorithm\n",
      "is then used to build the clusters. We present an evaluation framework for this task, analyze and discuss\n",
      "the results we obtained and propose directions for future works.\n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "Résumé\n",
      "\n",
      "En recherche d'information multilingue, beaucoup de travaux ont été menés sur les moteurs de recherche\n",
      "et sur les systémes de catégorisation automatique de texte. Dans cet article, nous abordons le probleme de\n",
      "découverte automatique de classes de documents dans une collection multilingue. Le but est d'extraire des\n",
      "documents de langues différentes traitant d'un méme sujet. Notre approche se fonde sur une analyse\n",
      "linguistique des documents permettant dobtenir une représentation vectorielle des documents, chaque\n",
      "langue étant représentée dans un espace vectorel différent, Nous proposons une mesure de similarté\n",
      "\n",
      "igue sur ces documents, utilisant des dictionnaires bilingues. Un algorithme de classification du\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "type ‘plus proches voisins’ est alors utilisé pour construire les classes. Nous présentons également une\n",
      "methode dévaluation ainsi que les résultats obtenus. Ces résultats seromt Giscutés et nous envisagerons\n",
      "plusieurs axes d’amélioration du systéme.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Information technology and globalization generate more and more electronic documents written in\n",
      "different languages. Dealing with such an amount of data requires automatic systems to filter, retrieve and\n",
      "classify multilingual documents. The Cross Language Information Retrieval community has designed\n",
      "multilingual search engines and text categorization systems that help users with specific needs. Search\n",
      "engines retrieve documents related to a specific user need expressed by a query, text categorization\n",
      "systems are used to assign to each document one of several predefined categories.\n",
      "\n",
      "Several approaches have been considered to solve the cross-language information retrieval problem.\n",
      "Search engines translate the user query in all indexed languages, then retrieve documents of different\n",
      "languages and merge results (Savoy J. 2002; Fluhr C. et al. 1997). Query translation can be achieved by\n",
      "the simple use of dictionaries (no disambiguation), or by machine translation (with disambiguation). An\n",
      "alternative way for query translation is Latent Semantic Indexing (Littman M. et al. 1997), which\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import pytesseract \n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Users/hp/Anaconda3/tesseract.exe'\n",
    "\n",
    "img = cv2.imread('b1.png')\n",
    "text1 = pytesseract.image_to_string(img)\n",
    "\n",
    "text = [text1]\n",
    "print(text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
      "Building wheels for collected packages: PyPDF2\n",
      "  Building wheel for PyPDF2 (setup.py): started\n",
      "  Building wheel for PyPDF2 (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\hp\\AppData\\Local\\pip\\Cache\\wheels\\53\\84\\19\\35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
      "Successfully built PyPDF2\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No. Of Pages : <class 'int'>\n",
      "Aman\n",
      " \n",
      "Shrivastava\n",
      " \n",
      "+91\n",
      "-\n",
      "8819870703\n",
      " \n",
      "amanshrivastav\n",
      "a10@gmail.com\n",
      " \n",
      "469/8\n",
      ",\n",
      " \n",
      "Janki nagar\n",
      ",\n",
      " \n",
      "Jabalpur,\n",
      " \n",
      "M.P. 482002\n",
      " \n",
      " \n",
      " \n",
      "SUMMARY:\n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "    \n",
      "An outlier \n",
      "Data Scientist with 2 years of \n",
      "experience in Data \n",
      "Science and Machine learning projects\n",
      ". \n",
      "My career objective is to enhance my skills in an \n",
      "innovative\n",
      " \n",
      "and dynamic workspace that gives me \n",
      "opportunity to find unobvious and smart solutions to obvious and challenging problems and thus \n",
      "help me grow as a profess\n",
      "ional and an intellectual. I am having an overall working experience of 5+ \n",
      "years working in TCS.\n",
      " \n",
      "\n",
      " \n",
      "    \n",
      "Efficient in performing data pre\n",
      "-\n",
      "processing on large data set.\n",
      " \n",
      "\n",
      " \n",
      "   \n",
      " \n",
      "Have good knowledge of working with various Machine learning models and dealing with \n",
      "ove\n",
      "r\n",
      "-\n",
      "fitting problems using sklearn libraries.\n",
      " \n",
      "\n",
      " \n",
      "    \n",
      "Have good statistical knowledge required to understand the distribution of the graphs and decision \n",
      "making.\n",
      " \n",
      "\n",
      " \n",
      "    \n",
      "Good experience on data visualization using matplotlib and seaborn libraries and have good \n",
      "knowle\n",
      "dge on tools like tableau for visualization.\n",
      " \n",
      "\n",
      " \n",
      "   \n",
      "Have experience working on OCR using pytesseract to covert the text in image to string form\n",
      ".\n",
      " \n",
      " \n",
      " \n",
      "KEY SKILLS\n",
      ":\n",
      " \n",
      " \n",
      "Languages:\n",
      " \n",
      "Python (pandas, numPy, matplotlib, seaborn, scikit\n",
      "-\n",
      "learn), C++.\n",
      " \n",
      " \n",
      "Data \n",
      "Science:\n",
      " \n",
      "Data \n",
      "Pre\n",
      "-\n",
      "processing, Exploratory Data Analysis, Supervised Learning \n",
      "Classification and Regression, Unsupervised learning \n",
      "-\n",
      " \n",
      "Clustering, \n",
      "Statistical Methods for Decision making, Machine learning Algorithms, \n",
      "Data Visualization, Ensemble Techniques, Natural languag\n",
      "e \n",
      "processing, Optical character recognition using\n",
      " \n",
      "pytesseract.\n",
      " \n",
      "Database:\n",
      " \n",
      "SQL\n",
      " \n",
      "Tools and \n",
      "IDE:\n",
      " \n",
      "Jupyter Notebook, MySQL Workbench, Tableau, shell script.\n",
      " \n",
      " \n",
      "PROFESSIONAL SUMMARY\n",
      ":\n",
      " \n",
      "Tata Consultancy Services \n",
      "                                                          \n",
      "                        \n",
      "Sept 2015\n",
      " \n",
      "\n",
      "Present\n",
      " \n",
      " \n",
      "1.\n",
      " \n",
      "  \n",
      " \n",
      "US based banking project\n",
      ": (\n",
      "August 2018\n",
      " \n",
      "\n",
      " \n",
      "Present)\n",
      " \n",
      "\n",
      " \n",
      "Responsible \n",
      "for \n",
      "developing\n",
      " \n",
      "a\n",
      " \n",
      "POC model for identify\n",
      "ing the patterns of breached SLA on \n",
      "vari\n",
      "ous support groups.\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import  PyPDF2\n",
    " \n",
    "pdfFileObject = open(r\"C:\\Users\\hp\\Desktop\\all about resumes\\Aman Shrivastava.pdf\", 'rb')\n",
    " \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    " \n",
    "print(\" No. Of Pages :\", type(pdfReader.numPages))\n",
    "\n",
    "pageObject = pdfReader.getPage(0)\n",
    " \n",
    "print(pageObject.extractText())\n",
    " \n",
    "pdfFileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\hp\\\\Desktop\\\\all about resumes\\\\Aman Shrivastava.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-16c69c7d18a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpdfFileObject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\hp\\Desktop\\all about resumes\\Aman Shrivastava.pdf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\hp\\\\Desktop\\\\all about resumes\\\\Aman Shrivastava.pdf'"
     ]
    }
   ],
   "source": [
    "import  PyPDF2\n",
    "\n",
    "lis = []\n",
    " \n",
    "pdfFileObject = open(r\"C:\\Users\\hp\\Desktop\\all about resumes\\Aman Shrivastava.pdf\", 'rb')\n",
    "\n",
    "\n",
    " \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    "\n",
    "\n",
    " \n",
    "print(\" No. Of Pages :\", pdfReader.numPages)\n",
    "\n",
    "countt = pdfReader.numPages\n",
    "\n",
    "\n",
    "#print(lis[0])\n",
    "    \n",
    "while i in pdfReader.numPages:\n",
    "    pageObject = pdfReader.getPage(0)\n",
    "    lis.append(pageObject)\n",
    " \n",
    "\n",
    " #print(pageObject.extractText())\n",
    "    \n",
    "pdfFileObject.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No. Of Pages : <class 'int'>\n",
      "No.\n",
      "Description\n",
      "Posted \n",
      "Shipment \n",
      "Date\n",
      "Direct \n",
      "Debit To \n",
      "PLA / RG\n",
      "Quantity\n",
      "Unit of \n",
      "Measure\n",
      "Unit Price\n",
      "Discount \n",
      "%\n",
      "Line \n",
      "Discount \n",
      "Amount\n",
      "Amount\n",
      "1906-S\n",
      "ATHENS Mobile Pedestal\n",
      "27-01-22\n",
      "No\n",
      "10\n",
      "stück\n",
      "363.661\n",
      "0.00\n",
      "3,636.61\n",
      "1908-S\n",
      "LONDON Swivel Chair, \n",
      "blue\n",
      "27-01-22\n",
      "No\n",
      "10\n",
      "stück\n",
      "159.311\n",
      "0.00\n",
      "1,593.11\n",
      "Total EUR\n",
      "5,229.72\n",
      "Other Taxes Amount\n",
      "0.00\n",
      "Charges Amount\n",
      "0.00\n",
      "Prices Including VAT\n",
      "Invoice No.\n",
      "Posting Date\n",
      "Bill-to Customer No.\n",
      "Account No.\n",
      "Bank\n",
      "Giro No.\n",
      "VAT Registration No.\n",
      "Home Page\n",
      "Phone No.\n",
      "No\n",
      "103027\n",
      "John Roberts\n",
      "Salesperson\n",
      "27. January 2022\n",
      "525252141\n",
      "VAT Registration No.\n",
      "49633663\n",
      "99-99-888\n",
      "World Wide Bank\n",
      "888-9999\n",
      "IN1234567890\n",
      "9865412538988\n",
      "700015 New York\n",
      "USA\n",
      "Radhe Nagar\n",
      "Hamburg 36, DE-22417\n",
      "The Ring 6\n",
      "Porschestraße 911\n",
      "DHANRAM ENTERPRISES\n",
      "Cronus USA, Inc\n",
      "Sales - Invoice\n",
      "Page 1 of 2\n",
      "27. January 2022\n",
      "Due Date\n",
      "10. February 2022\n",
      "E-Mail\n",
      "Payment Terms\n",
      "Net 14 days\n",
      "Shipment Method\n",
      "Ex Warehouse\n",
      "Document Date\n",
      "GJX118910100578\n",
      "Service Tax \n",
      "Registration No.\n",
      "Company Registration No.\n",
      "07COMPA0007I1Z3\n",
      "Customer GST Reg No.\n",
      "Ship-to GST Reg. No.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import  PyPDF2\n",
    " \n",
    "pdfFileObject = open(r\"F:\\BLOCKNIT\\New Folder\\dhanram enterprises.pdf\", 'rb')\n",
    " \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    " \n",
    "print(\" No. Of Pages :\", type(pdfReader.numPages))\n",
    "\n",
    "pageObject = pdfReader.getPage(0)\n",
    " \n",
    "print(pageObject.extractText())\n",
    " \n",
    "pdfFileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 11-12: malformed \\N character escape (<ipython-input-6-53de39228952>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-53de39228952>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    img = cv2.imread('F:\\BLOCKNIT\\New Folder\\SampleJPEGInvoice.jpg')\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 11-12: malformed \\N character escape\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import pytesseract \n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Users/hp/Anaconda3/tesseract.exe'\n",
    "\n",
    "img = cv2.imread('F:\\BLOCKNIT\\New Folder\\SampleJPEGInvoice.jpg')\n",
    "text1 = pytesseract.image_to_string(img)\n",
    "\n",
    "text = [text1]\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
